{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('amazonreviews.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   pos  Stuning even for the non-gamer: This sound tra...\n",
       "1   pos  The best soundtrack ever to anything.: I'm rea...\n",
       "2   pos  Amazing!: This soundtrack is my favorite music...\n",
       "3   pos  Excellent Soundtrack: I truly like this soundt...\n",
       "4   pos  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=enc.fit_transform(df.label)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.review.values[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "reviews=[]\n",
    "for t in X:\n",
    "        reviews.append([i.lemma_ for i in nlp(strip_punctuation(t.lower())) if not i.is_space and not i.like_url and not i.is_stop and not i.like_email and len(i.lemma_)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def loadData(file): \n",
    "    # for reading also binary mode is important \n",
    "    dbfile = open(file, 'rb')      \n",
    "    db = pkl.load(dbfile) \n",
    "    dbfile.close()\n",
    "    return db\n",
    "def saveData(file,data): \n",
    "    # for reading also binary mode is important \n",
    "    dbfile = open(file, 'ab')      \n",
    "    db = pkl.dump(data,dbfile) \n",
    "    dbfile.close()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#TF-IDF # needs a list of lists for words and docs along with a fasttext 'model'\n",
    "text = []\n",
    "for i in reviews:\n",
    "    string = ' '.join(i)\n",
    "    text.append(string)\n",
    "tf_idf_vect = TfidfVectorizer(stop_words=None)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(text)\n",
    "tfidf_feat = tf_idf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x6646 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30365 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model_ft = FastText(reviews, size=20, window=5, min_count=1, iter=10, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging Fast Text Embeddings to get Doc Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_g=[]\n",
    "for r in reviews:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        sum_words+=model_ft.wv[w]\n",
    "    X_g.append(sum_words/num_words)\n",
    "X_g=np.array(X_g)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_g,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.57        87\n",
      "           1       0.48      0.45      0.46        73\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       160\n",
      "   macro avg       0.52      0.52      0.52       160\n",
      "weighted avg       0.52      0.53      0.52       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56        87\n",
      "           1       0.49      0.51      0.50        73\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       160\n",
      "   macro avg       0.53      0.53      0.53       160\n",
      "weighted avg       0.53      0.53      0.53       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        87\n",
      "           1       0.00      0.00      0.00        73\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       160\n",
      "   macro avg       0.27      0.50      0.35       160\n",
      "weighted avg       0.30      0.54      0.38       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,roc_auc_score\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining TF-IDF with FastText Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_ft = [] # the tfidf-ft for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(20) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = model_ft.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_ft.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft=np.array(tfidf_sent_vectors_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_ft,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.91      0.65        77\n",
      "           1       0.67      0.17      0.27        83\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       160\n",
      "   macro avg       0.59      0.54      0.46       160\n",
      "weighted avg       0.59      0.53      0.45       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.64      0.56        77\n",
      "           1       0.55      0.41      0.47        83\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       160\n",
      "   macro avg       0.52      0.52      0.51       160\n",
      "weighted avg       0.53      0.52      0.51       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.66      0.59        77\n",
      "           1       0.59      0.45      0.51        83\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       160\n",
      "   macro avg       0.56      0.55      0.55       160\n",
      "weighted avg       0.56      0.55      0.55       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 15:23:28.380835 12412 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v=Word2Vec(reviews,size=20,window=5,sg=1,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging SkipGram Text Embeddings to get Doc Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_g=[]\n",
    "for r in reviews:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        sum_words+=w2v.wv[w]\n",
    "    X_g.append(sum_words/num_words)\n",
    "X_g=np.array(X_g)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_g,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        89\n",
      "           1       1.00      0.01      0.03        71\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       160\n",
      "   macro avg       0.78      0.51      0.37       160\n",
      "weighted avg       0.76      0.56      0.41       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.71      0.66        89\n",
      "           1       0.54      0.44      0.48        71\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       160\n",
      "   macro avg       0.58      0.57      0.57       160\n",
      "weighted avg       0.58      0.59      0.58       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.69      0.66        89\n",
      "           1       0.56      0.51      0.53        71\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       160\n",
      "   macro avg       0.60      0.60      0.60       160\n",
      "weighted avg       0.60      0.61      0.60       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining TF-IDF with Word2Vec SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.99      0.68        83\n",
      "           1       0.50      0.01      0.03        77\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       160\n",
      "   macro avg       0.51      0.50      0.35       160\n",
      "weighted avg       0.51      0.52      0.37       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60        83\n",
      "           1       0.55      0.48      0.51        77\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       160\n",
      "   macro avg       0.56      0.56      0.56       160\n",
      "weighted avg       0.56      0.56      0.56       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        83\n",
      "           1       0.54      0.55      0.54        77\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       160\n",
      "   macro avg       0.56      0.56      0.56       160\n",
      "weighted avg       0.56      0.56      0.56       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_sk = [] # the tfidf-sk for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(20) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_sk.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))\n",
    "\n",
    "X_sk=np.array(tfidf_sent_vectors_sk)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_sk,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 15:23:37.351999 12412 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "w2v_cbow=Word2Vec(reviews,size=20,window=5,sg=0,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging CBOW Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73        91\n",
      "           1       0.00      0.00      0.00        69\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       160\n",
      "   macro avg       0.28      0.50      0.36       160\n",
      "weighted avg       0.32      0.57      0.41       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.62      0.61        91\n",
      "           1       0.49      0.48      0.48        69\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       160\n",
      "   macro avg       0.55      0.55      0.55       160\n",
      "weighted avg       0.56      0.56      0.56       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        91\n",
      "           1       0.43      0.42      0.42        69\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       160\n",
      "   macro avg       0.50      0.50      0.50       160\n",
      "weighted avg       0.51      0.51      0.51       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_g=[]\n",
    "for r in reviews:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        sum_words+=w2v_cbow.wv[w]\n",
    "    X_g.append(sum_words/num_words)\n",
    "X_g=np.array(X_g)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_g,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Word Vectors with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.68        82\n",
      "           1       0.00      0.00      0.00        78\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       160\n",
      "   macro avg       0.26      0.50      0.34       160\n",
      "weighted avg       0.26      0.51      0.35       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.65      0.56        82\n",
      "           1       0.46      0.32      0.38        78\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       160\n",
      "   macro avg       0.48      0.48      0.47       160\n",
      "weighted avg       0.48      0.49      0.47       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.66      0.60        82\n",
      "           1       0.54      0.42      0.47        78\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       160\n",
      "   macro avg       0.54      0.54      0.54       160\n",
      "weighted avg       0.54      0.54      0.54       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_cbow = [] # the tfidf-sk for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(20) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_cbow.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_cbow.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))\n",
    "\n",
    "X_cbow=np.array(tfidf_sent_vectors_cbow)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_cbow,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text for Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['PAD']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max([len(s) for s in reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = tokenizer.word_index\n",
    "word2id['PAD']=0\n",
    "id2word={v:k for k,v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs=tokenizer.texts_to_sequences(reviews)\n",
    "X=pad_sequences(revs,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_idx=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# co-occurence matrix\n",
    "X = np.zeros((V, V))\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "context_size=5\n",
    "for s in rev_idx:\n",
    "    for i in range(len(s)):\n",
    "        wi=s[i] # select current word\n",
    "        start= max(0,i-context_size) # define start index\n",
    "        end = min(100,i+context_size) # define end index of the context\n",
    "\n",
    "        if i - context_size < 0:\n",
    "            points = 1.0/(i+1) # calculate context distances \n",
    "            X[wi,0]+=points\n",
    "            X[0,wi]+=points\n",
    "            \n",
    "        if i + context_size > 100:\n",
    "            points = 1.0 / (100 - i)\n",
    "            X[wi,1] += points\n",
    "            X[1,wi] += points\n",
    "\n",
    "        for j in range(start,i):\n",
    "            wj = s[j]\n",
    "            points = 1.0 / (i - j) # this is +ve\n",
    "            X[wi,wj] += points\n",
    "            X[wj,wi] += points\n",
    "            \n",
    "        # right side\n",
    "        for j in range(i + 1, end):\n",
    "            wj = s[j]\n",
    "            points = 1.0 / (j - i) # this is +ve\n",
    "            X[wi,wj] += points\n",
    "            X[wj,wi] += points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight matrix\n",
    "fX=np.zeros((V,V))\n",
    "fX[X<100]=(X[X<100]/float(100))**0.75\n",
    "fX[X>=100]=1\n",
    "# target\n",
    "logX = np.log(X + 1)\n",
    "D=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(tf.keras.Model):\n",
    "    def __init__(self, num_dims, vocab_size,mu):\n",
    "        super(Glove, self).__init__()\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        self.mu = mu\n",
    "        # initialize weights, inputs, targets placeholders\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
    "        self.U = tf.Variable(U.astype(np.float32))\n",
    "        self.c = tf.Variable(c.reshape(1, V).astype(np.float32))\n",
    "        self.params = [self.W, self.b,self.U,self.c]\n",
    "\n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(self.W, tf.transpose(self.U)) + self.b + self.c + self.mu\n",
    "\n",
    "# Define the loss\n",
    "def get_loss(model, inputs, targets):\n",
    "    predictions = model(inputs)\n",
    "    delta = targets - predictions\n",
    "    return tf.reduce_sum(inputs * delta * delta)\n",
    "\n",
    "# Gradient function\n",
    "def get_grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # calculate the loss\n",
    "        loss_value = get_loss(model, inputs, targets)\n",
    "        # return gradient\n",
    "        return tape.gradient(loss_value, model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = logX.mean()\n",
    "glove_model=Glove(20,V,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the losses here\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0916 18:33:45.195358 14080 base_layer.py:1772] Layer glove_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   tf.Tensor(13577.893, shape=(), dtype=float32)\n",
      "1   tf.Tensor(13189.344, shape=(), dtype=float32)\n",
      "2   tf.Tensor(12783.364, shape=(), dtype=float32)\n",
      "3   tf.Tensor(12351.918, shape=(), dtype=float32)\n",
      "4   tf.Tensor(11891.551, shape=(), dtype=float32)\n",
      "5   tf.Tensor(11401.018, shape=(), dtype=float32)\n",
      "6   tf.Tensor(10880.328, shape=(), dtype=float32)\n",
      "7   tf.Tensor(10330.77, shape=(), dtype=float32)\n",
      "8   tf.Tensor(9755.085, shape=(), dtype=float32)\n",
      "9   tf.Tensor(9157.549, shape=(), dtype=float32)\n",
      "10   tf.Tensor(8544.055, shape=(), dtype=float32)\n",
      "11   tf.Tensor(7922.18, shape=(), dtype=float32)\n",
      "12   tf.Tensor(7301.2637, shape=(), dtype=float32)\n",
      "13   tf.Tensor(6692.4316, shape=(), dtype=float32)\n",
      "14   tf.Tensor(6108.5107, shape=(), dtype=float32)\n",
      "15   tf.Tensor(5563.7354, shape=(), dtype=float32)\n",
      "16   tf.Tensor(5073.0103, shape=(), dtype=float32)\n",
      "17   tf.Tensor(4650.4575, shape=(), dtype=float32)\n",
      "18   tf.Tensor(4306.875, shape=(), dtype=float32)\n",
      "19   tf.Tensor(4046.212, shape=(), dtype=float32)\n",
      "20   tf.Tensor(3862.3665, shape=(), dtype=float32)\n",
      "21   tf.Tensor(3738.7388, shape=(), dtype=float32)\n",
      "22   tf.Tensor(3651.9138, shape=(), dtype=float32)\n",
      "23   tf.Tensor(3577.8066, shape=(), dtype=float32)\n",
      "24   tf.Tensor(3497.1218, shape=(), dtype=float32)\n",
      "25   tf.Tensor(3398.1702, shape=(), dtype=float32)\n",
      "26   tf.Tensor(3277.035, shape=(), dtype=float32)\n",
      "27   tf.Tensor(3136.0713, shape=(), dtype=float32)\n",
      "28   tf.Tensor(2981.7366, shape=(), dtype=float32)\n",
      "29   tf.Tensor(2822.4114, shape=(), dtype=float32)\n",
      "30   tf.Tensor(2666.5872, shape=(), dtype=float32)\n",
      "31   tf.Tensor(2521.5374, shape=(), dtype=float32)\n",
      "32   tf.Tensor(2392.5317, shape=(), dtype=float32)\n",
      "33   tf.Tensor(2282.5232, shape=(), dtype=float32)\n",
      "34   tf.Tensor(2192.2283, shape=(), dtype=float32)\n",
      "35   tf.Tensor(2120.4846, shape=(), dtype=float32)\n",
      "36   tf.Tensor(2064.7668, shape=(), dtype=float32)\n",
      "37   tf.Tensor(2021.747, shape=(), dtype=float32)\n",
      "38   tf.Tensor(1987.8203, shape=(), dtype=float32)\n",
      "39   tf.Tensor(1959.535, shape=(), dtype=float32)\n",
      "40   tf.Tensor(1933.9038, shape=(), dtype=float32)\n",
      "41   tf.Tensor(1908.5935, shape=(), dtype=float32)\n",
      "42   tf.Tensor(1882.0048, shape=(), dtype=float32)\n",
      "43   tf.Tensor(1853.2676, shape=(), dtype=float32)\n",
      "44   tf.Tensor(1822.1681, shape=(), dtype=float32)\n",
      "45   tf.Tensor(1789.0316, shape=(), dtype=float32)\n",
      "46   tf.Tensor(1754.5792, shape=(), dtype=float32)\n",
      "47   tf.Tensor(1719.7678, shape=(), dtype=float32)\n",
      "48   tf.Tensor(1685.6343, shape=(), dtype=float32)\n",
      "49   tf.Tensor(1653.1481, shape=(), dtype=float32)\n",
      "50   tf.Tensor(1623.0919, shape=(), dtype=float32)\n",
      "51   tf.Tensor(1595.9763, shape=(), dtype=float32)\n",
      "52   tf.Tensor(1571.9973, shape=(), dtype=float32)\n",
      "53   tf.Tensor(1551.0409, shape=(), dtype=float32)\n",
      "54   tf.Tensor(1532.7327, shape=(), dtype=float32)\n",
      "55   tf.Tensor(1516.518, shape=(), dtype=float32)\n",
      "56   tf.Tensor(1501.7643, shape=(), dtype=float32)\n",
      "57   tf.Tensor(1487.8612, shape=(), dtype=float32)\n",
      "58   tf.Tensor(1474.3031, shape=(), dtype=float32)\n",
      "59   tf.Tensor(1460.7452, shape=(), dtype=float32)\n",
      "60   tf.Tensor(1447.0203, shape=(), dtype=float32)\n",
      "61   tf.Tensor(1433.1227, shape=(), dtype=float32)\n",
      "62   tf.Tensor(1419.1681, shape=(), dtype=float32)\n",
      "63   tf.Tensor(1405.3378, shape=(), dtype=float32)\n",
      "64   tf.Tensor(1391.8254, shape=(), dtype=float32)\n",
      "65   tf.Tensor(1378.7913, shape=(), dtype=float32)\n",
      "66   tf.Tensor(1366.3337, shape=(), dtype=float32)\n",
      "67   tf.Tensor(1354.4795, shape=(), dtype=float32)\n",
      "68   tf.Tensor(1343.1901, shape=(), dtype=float32)\n",
      "69   tf.Tensor(1332.3774, shape=(), dtype=float32)\n",
      "70   tf.Tensor(1321.9293, shape=(), dtype=float32)\n",
      "71   tf.Tensor(1311.7305, shape=(), dtype=float32)\n",
      "72   tf.Tensor(1301.6815, shape=(), dtype=float32)\n",
      "73   tf.Tensor(1291.7123, shape=(), dtype=float32)\n",
      "74   tf.Tensor(1281.7838, shape=(), dtype=float32)\n",
      "75   tf.Tensor(1271.888, shape=(), dtype=float32)\n",
      "76   tf.Tensor(1262.0392, shape=(), dtype=float32)\n",
      "77   tf.Tensor(1252.2646, shape=(), dtype=float32)\n",
      "78   tf.Tensor(1242.5951, shape=(), dtype=float32)\n",
      "79   tf.Tensor(1233.0562, shape=(), dtype=float32)\n",
      "80   tf.Tensor(1223.6635, shape=(), dtype=float32)\n",
      "81   tf.Tensor(1214.4209, shape=(), dtype=float32)\n",
      "82   tf.Tensor(1205.3203, shape=(), dtype=float32)\n",
      "83   tf.Tensor(1196.3461, shape=(), dtype=float32)\n",
      "84   tf.Tensor(1187.4783, shape=(), dtype=float32)\n",
      "85   tf.Tensor(1178.697, shape=(), dtype=float32)\n",
      "86   tf.Tensor(1169.9858, shape=(), dtype=float32)\n",
      "87   tf.Tensor(1161.333, shape=(), dtype=float32)\n",
      "88   tf.Tensor(1152.7336, shape=(), dtype=float32)\n",
      "89   tf.Tensor(1144.1876, shape=(), dtype=float32)\n",
      "90   tf.Tensor(1135.6981, shape=(), dtype=float32)\n",
      "91   tf.Tensor(1127.272, shape=(), dtype=float32)\n",
      "92   tf.Tensor(1118.9148, shape=(), dtype=float32)\n",
      "93   tf.Tensor(1110.6322, shape=(), dtype=float32)\n",
      "94   tf.Tensor(1102.4268, shape=(), dtype=float32)\n",
      "95   tf.Tensor(1094.3002, shape=(), dtype=float32)\n",
      "96   tf.Tensor(1086.2512, shape=(), dtype=float32)\n",
      "97   tf.Tensor(1078.2778, shape=(), dtype=float32)\n",
      "98   tf.Tensor(1070.3779, shape=(), dtype=float32)\n",
      "99   tf.Tensor(1062.5482, shape=(), dtype=float32)\n",
      "100   tf.Tensor(1054.7875, shape=(), dtype=float32)\n",
      "101   tf.Tensor(1047.095, shape=(), dtype=float32)\n",
      "102   tf.Tensor(1039.4706, shape=(), dtype=float32)\n",
      "103   tf.Tensor(1031.9161, shape=(), dtype=float32)\n",
      "104   tf.Tensor(1024.4332, shape=(), dtype=float32)\n",
      "105   tf.Tensor(1017.0238, shape=(), dtype=float32)\n",
      "106   tf.Tensor(1009.6896, shape=(), dtype=float32)\n",
      "107   tf.Tensor(1002.4318, shape=(), dtype=float32)\n",
      "108   tf.Tensor(995.25134, shape=(), dtype=float32)\n",
      "109   tf.Tensor(988.14734, shape=(), dtype=float32)\n",
      "110   tf.Tensor(981.1198, shape=(), dtype=float32)\n",
      "111   tf.Tensor(974.16754, shape=(), dtype=float32)\n",
      "112   tf.Tensor(967.28937, shape=(), dtype=float32)\n",
      "113   tf.Tensor(960.4845, shape=(), dtype=float32)\n",
      "114   tf.Tensor(953.7522, shape=(), dtype=float32)\n",
      "115   tf.Tensor(947.0921, shape=(), dtype=float32)\n",
      "116   tf.Tensor(940.5038, shape=(), dtype=float32)\n",
      "117   tf.Tensor(933.9874, shape=(), dtype=float32)\n",
      "118   tf.Tensor(927.54266, shape=(), dtype=float32)\n",
      "119   tf.Tensor(921.1692, shape=(), dtype=float32)\n",
      "120   tf.Tensor(914.8665, shape=(), dtype=float32)\n",
      "121   tf.Tensor(908.63434, shape=(), dtype=float32)\n",
      "122   tf.Tensor(902.4715, shape=(), dtype=float32)\n",
      "123   tf.Tensor(896.37683, shape=(), dtype=float32)\n",
      "124   tf.Tensor(890.3494, shape=(), dtype=float32)\n",
      "125   tf.Tensor(884.3884, shape=(), dtype=float32)\n",
      "126   tf.Tensor(878.4926, shape=(), dtype=float32)\n",
      "127   tf.Tensor(872.6613, shape=(), dtype=float32)\n",
      "128   tf.Tensor(866.8936, shape=(), dtype=float32)\n",
      "129   tf.Tensor(861.189, shape=(), dtype=float32)\n",
      "130   tf.Tensor(855.5466, shape=(), dtype=float32)\n",
      "131   tf.Tensor(849.96576, shape=(), dtype=float32)\n",
      "132   tf.Tensor(844.44604, shape=(), dtype=float32)\n",
      "133   tf.Tensor(838.9866, shape=(), dtype=float32)\n",
      "134   tf.Tensor(833.58704, shape=(), dtype=float32)\n",
      "135   tf.Tensor(828.2463, shape=(), dtype=float32)\n",
      "136   tf.Tensor(822.9637, shape=(), dtype=float32)\n",
      "137   tf.Tensor(817.7388, shape=(), dtype=float32)\n",
      "138   tf.Tensor(812.5706, shape=(), dtype=float32)\n",
      "139   tf.Tensor(807.4588, shape=(), dtype=float32)\n",
      "140   tf.Tensor(802.4025, shape=(), dtype=float32)\n",
      "141   tf.Tensor(797.4014, shape=(), dtype=float32)\n",
      "142   tf.Tensor(792.45465, shape=(), dtype=float32)\n",
      "143   tf.Tensor(787.562, shape=(), dtype=float32)\n",
      "144   tf.Tensor(782.7226, shape=(), dtype=float32)\n",
      "145   tf.Tensor(777.93616, shape=(), dtype=float32)\n",
      "146   tf.Tensor(773.20184, shape=(), dtype=float32)\n",
      "147   tf.Tensor(768.51965, shape=(), dtype=float32)\n",
      "148   tf.Tensor(763.88873, shape=(), dtype=float32)\n",
      "149   tf.Tensor(759.3087, shape=(), dtype=float32)\n",
      "150   tf.Tensor(754.77905, shape=(), dtype=float32)\n",
      "151   tf.Tensor(750.29926, shape=(), dtype=float32)\n",
      "152   tf.Tensor(745.869, shape=(), dtype=float32)\n",
      "153   tf.Tensor(741.4878, shape=(), dtype=float32)\n",
      "154   tf.Tensor(737.15515, shape=(), dtype=float32)\n",
      "155   tf.Tensor(732.871, shape=(), dtype=float32)\n",
      "156   tf.Tensor(728.6346, shape=(), dtype=float32)\n",
      "157   tf.Tensor(724.4457, shape=(), dtype=float32)\n",
      "158   tf.Tensor(720.30396, shape=(), dtype=float32)\n",
      "159   tf.Tensor(716.20905, shape=(), dtype=float32)\n",
      "160   tf.Tensor(712.1604, shape=(), dtype=float32)\n",
      "161   tf.Tensor(708.1577, shape=(), dtype=float32)\n",
      "162   tf.Tensor(704.20074, shape=(), dtype=float32)\n",
      "163   tf.Tensor(700.28906, shape=(), dtype=float32)\n",
      "164   tf.Tensor(696.42224, shape=(), dtype=float32)\n",
      "165   tf.Tensor(692.6001, shape=(), dtype=float32)\n",
      "166   tf.Tensor(688.822, shape=(), dtype=float32)\n",
      "167   tf.Tensor(685.08777, shape=(), dtype=float32)\n",
      "168   tf.Tensor(681.39716, shape=(), dtype=float32)\n",
      "169   tf.Tensor(677.7496, shape=(), dtype=float32)\n",
      "170   tf.Tensor(674.1447, shape=(), dtype=float32)\n",
      "171   tf.Tensor(670.58234, shape=(), dtype=float32)\n",
      "172   tf.Tensor(667.06177, shape=(), dtype=float32)\n",
      "173   tf.Tensor(663.58295, shape=(), dtype=float32)\n",
      "174   tf.Tensor(660.1454, shape=(), dtype=float32)\n",
      "175   tf.Tensor(656.7486, shape=(), dtype=float32)\n",
      "176   tf.Tensor(653.3924, shape=(), dtype=float32)\n",
      "177   tf.Tensor(650.0763, shape=(), dtype=float32)\n",
      "178   tf.Tensor(646.7999, shape=(), dtype=float32)\n",
      "179   tf.Tensor(643.5628, shape=(), dtype=float32)\n",
      "180   tf.Tensor(640.36456, shape=(), dtype=float32)\n",
      "181   tf.Tensor(637.20496, shape=(), dtype=float32)\n",
      "182   tf.Tensor(634.08344, shape=(), dtype=float32)\n",
      "183   tf.Tensor(630.9997, shape=(), dtype=float32)\n",
      "184   tf.Tensor(627.95325, shape=(), dtype=float32)\n",
      "185   tf.Tensor(624.9438, shape=(), dtype=float32)\n",
      "186   tf.Tensor(621.9709, shape=(), dtype=float32)\n",
      "187   tf.Tensor(619.0341, shape=(), dtype=float32)\n",
      "188   tf.Tensor(616.1331, shape=(), dtype=float32)\n",
      "189   tf.Tensor(613.2674, shape=(), dtype=float32)\n",
      "190   tf.Tensor(610.4367, shape=(), dtype=float32)\n",
      "191   tf.Tensor(607.6406, shape=(), dtype=float32)\n",
      "192   tf.Tensor(604.8786, shape=(), dtype=float32)\n",
      "193   tf.Tensor(602.15045, shape=(), dtype=float32)\n",
      "194   tf.Tensor(599.4556, shape=(), dtype=float32)\n",
      "195   tf.Tensor(596.79376, shape=(), dtype=float32)\n",
      "196   tf.Tensor(594.16455, shape=(), dtype=float32)\n",
      "197   tf.Tensor(591.5675, shape=(), dtype=float32)\n",
      "198   tf.Tensor(589.0022, shape=(), dtype=float32)\n",
      "199   tf.Tensor(586.46844, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Run the training loop\n",
    "for i in range(200):\n",
    "    # Get gradients\n",
    "    grads = get_grad(glove_model, fX, logX)\n",
    "\n",
    "    # Do one step of gradient descent: param <- param - learning_rate * grad\n",
    "    optimizer.apply_gradients(zip(grads, glove_model.params))\n",
    "\n",
    "    # Store the loss\n",
    "    loss = get_loss(glove_model, fX, logX)\n",
    "    losses.append(loss)\n",
    "    print(i,\" \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_f,_,U_f,_=glove_model.params\n",
    "W1,W2=[W_f.numpy(),U_f.numpy().T]\n",
    "We = np.hstack([W1, W2.T])\n",
    "We_avg = (W1 + W2.T) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6647, 20)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "We_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb_avg=loadData('glove_avg.pkl')\n",
    "glove_emb_concat=loadData('glove_concat.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging GloVe Embeddings to get Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_g=[]\n",
    "for r in revs:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        sum_words+=glove_emb_avg[w]\n",
    "    X_g.append(sum_words/num_words)\n",
    "X_g=np.array(X_g)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_g,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.81      0.68        81\n",
      "           1       0.68      0.41      0.51        79\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       160\n",
      "   macro avg       0.63      0.61      0.59       160\n",
      "weighted avg       0.63      0.61      0.60       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.78      0.68        81\n",
      "           1       0.67      0.47      0.55        79\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       160\n",
      "   macro avg       0.64      0.62      0.61       160\n",
      "weighted avg       0.64      0.62      0.62       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.62        81\n",
      "           1       0.60      0.54      0.57        79\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       160\n",
      "   macro avg       0.59      0.59      0.59       160\n",
      "weighted avg       0.59      0.59      0.59       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concated GloVe Embeddings which are averaged to get Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.73      0.69        84\n",
      "           1       0.65      0.57      0.61        76\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       160\n",
      "   macro avg       0.65      0.65      0.65       160\n",
      "weighted avg       0.65      0.65      0.65       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69        84\n",
      "           1       0.66      0.54      0.59        76\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       160\n",
      "   macro avg       0.65      0.64      0.64       160\n",
      "weighted avg       0.65      0.65      0.65       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.68        84\n",
      "           1       0.65      0.58      0.61        76\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       160\n",
      "   macro avg       0.65      0.65      0.65       160\n",
      "weighted avg       0.65      0.65      0.65       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_g=[]\n",
    "for r in revs:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        sum_words+=glove_emb_concat[w]\n",
    "    X_g.append(sum_words/num_words)\n",
    "X_g=np.array(X_g)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_g,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining GloVe with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Averaged GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_gl = [] # the tfidf-sk for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(20) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = glove_emb_avg[word2id[word]]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_gl.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.79      0.66        82\n",
      "           1       0.62      0.36      0.46        78\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       160\n",
      "   macro avg       0.59      0.58      0.56       160\n",
      "weighted avg       0.59      0.58      0.56       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67        82\n",
      "           1       0.65      0.47      0.55        78\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       160\n",
      "   macro avg       0.63      0.62      0.61       160\n",
      "weighted avg       0.62      0.62      0.61       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61        82\n",
      "           1       0.58      0.49      0.53        78\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       160\n",
      "   macro avg       0.58      0.57      0.57       160\n",
      "weighted avg       0.58      0.57      0.57       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_skip=np.array(tfidf_sent_vectors_gl)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_skip,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Concatenated GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_gl = [] # the tfidf-sk for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(40) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = glove_emb_concat[word2id[word]]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_gl.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.71      0.63        85\n",
      "           1       0.54      0.39      0.45        75\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       160\n",
      "   macro avg       0.55      0.55      0.54       160\n",
      "weighted avg       0.55      0.56      0.54       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62        85\n",
      "           1       0.52      0.37      0.43        75\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       160\n",
      "   macro avg       0.54      0.53      0.53       160\n",
      "weighted avg       0.54      0.54      0.53       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70        85\n",
      "           1       0.66      0.57      0.61        75\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       160\n",
      "   macro avg       0.66      0.66      0.66       160\n",
      "weighted avg       0.66      0.66      0.66       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_skip=np.array(tfidf_sent_vectors_gl)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_skip,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointly Learnt Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,Flatten,LSTM,Dropout,GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 20)           132940    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 5)            520       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 15)                90        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 80        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 133,636\n",
      "Trainable params: 133,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=V,output_dim=20,input_length=max_len))\n",
    "model.add(LSTM(5,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(15, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss = \"mse\", optimizer = opt, metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/20\n",
      "640/640 [==============================] - ETA: 1:20 - loss: 0.2506 - accuracy: 0.40 - ETA: 24s - loss: 0.2504 - accuracy: 0.4271 - ETA: 17s - loss: 0.2505 - accuracy: 0.429 - ETA: 13s - loss: 0.2501 - accuracy: 0.443 - ETA: 10s - loss: 0.2501 - accuracy: 0.453 - ETA: 6s - loss: 0.2500 - accuracy: 0.464 - ETA: 5s - loss: 0.2504 - accuracy: 0.46 - ETA: 4s - loss: 0.2501 - accuracy: 0.47 - ETA: 3s - loss: 0.2494 - accuracy: 0.49 - ETA: 2s - loss: 0.2500 - accuracy: 0.48 - ETA: 1s - loss: 0.2508 - accuracy: 0.48 - ETA: 0s - loss: 0.2505 - accuracy: 0.48 - ETA: 0s - loss: 0.2505 - accuracy: 0.47 - 6s 9ms/sample - loss: 0.2503 - accuracy: 0.4828 - val_loss: 0.2462 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.59 - ETA: 0s - loss: 0.2481 - accuracy: 0.53 - ETA: 0s - loss: 0.2482 - accuracy: 0.51 - ETA: 0s - loss: 0.2494 - accuracy: 0.49 - ETA: 0s - loss: 0.2487 - accuracy: 0.48 - ETA: 0s - loss: 0.2475 - accuracy: 0.50 - ETA: 0s - loss: 0.2484 - accuracy: 0.50 - ETA: 0s - loss: 0.2485 - accuracy: 0.50 - ETA: 0s - loss: 0.2479 - accuracy: 0.51 - ETA: 0s - loss: 0.2479 - accuracy: 0.51 - ETA: 0s - loss: 0.2478 - accuracy: 0.51 - ETA: 0s - loss: 0.2492 - accuracy: 0.50 - ETA: 0s - loss: 0.2489 - accuracy: 0.50 - ETA: 0s - loss: 0.2485 - accuracy: 0.51 - 1s 2ms/sample - loss: 0.2485 - accuracy: 0.5109 - val_loss: 0.2431 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.56 - ETA: 0s - loss: 0.2397 - accuracy: 0.51 - ETA: 0s - loss: 0.2397 - accuracy: 0.57 - ETA: 0s - loss: 0.2422 - accuracy: 0.56 - ETA: 0s - loss: 0.2428 - accuracy: 0.54 - ETA: 0s - loss: 0.2400 - accuracy: 0.56 - ETA: 0s - loss: 0.2408 - accuracy: 0.56 - ETA: 0s - loss: 0.2408 - accuracy: 0.55 - ETA: 0s - loss: 0.2421 - accuracy: 0.54 - ETA: 0s - loss: 0.2418 - accuracy: 0.54 - ETA: 0s - loss: 0.2416 - accuracy: 0.54 - ETA: 0s - loss: 0.2400 - accuracy: 0.55 - ETA: 0s - loss: 0.2403 - accuracy: 0.54 - 1s 2ms/sample - loss: 0.2402 - accuracy: 0.5484 - val_loss: 0.2334 - val_accuracy: 0.6812\n",
      "Epoch 4/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.65 - ETA: 0s - loss: 0.2106 - accuracy: 0.62 - ETA: 0s - loss: 0.2198 - accuracy: 0.59 - ETA: 0s - loss: 0.2158 - accuracy: 0.64 - ETA: 0s - loss: 0.2084 - accuracy: 0.67 - ETA: 0s - loss: 0.2046 - accuracy: 0.69 - ETA: 0s - loss: 0.2031 - accuracy: 0.70 - ETA: 0s - loss: 0.2000 - accuracy: 0.71 - ETA: 0s - loss: 0.1969 - accuracy: 0.72 - ETA: 0s - loss: 0.1948 - accuracy: 0.71 - ETA: 0s - loss: 0.2016 - accuracy: 0.69 - ETA: 0s - loss: 0.2020 - accuracy: 0.69 - ETA: 0s - loss: 0.1982 - accuracy: 0.70 - ETA: 0s - loss: 0.2011 - accuracy: 0.69 - ETA: 0s - loss: 0.2003 - accuracy: 0.69 - ETA: 0s - loss: 0.1995 - accuracy: 0.69 - ETA: 0s - loss: 0.1985 - accuracy: 0.70 - 1s 2ms/sample - loss: 0.1963 - accuracy: 0.7047 - val_loss: 0.1570 - val_accuracy: 0.7750\n",
      "Epoch 5/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.84 - ETA: 1s - loss: 0.1259 - accuracy: 0.81 - ETA: 1s - loss: 0.1225 - accuracy: 0.82 - ETA: 0s - loss: 0.1264 - accuracy: 0.80 - ETA: 0s - loss: 0.1276 - accuracy: 0.82 - ETA: 0s - loss: 0.1408 - accuracy: 0.80 - ETA: 0s - loss: 0.1479 - accuracy: 0.78 - ETA: 0s - loss: 0.1514 - accuracy: 0.77 - ETA: 0s - loss: 0.1473 - accuracy: 0.77 - ETA: 0s - loss: 0.1512 - accuracy: 0.77 - ETA: 0s - loss: 0.1452 - accuracy: 0.79 - ETA: 0s - loss: 0.1397 - accuracy: 0.80 - ETA: 0s - loss: 0.1419 - accuracy: 0.79 - ETA: 0s - loss: 0.1446 - accuracy: 0.79 - ETA: 0s - loss: 0.1400 - accuracy: 0.79 - ETA: 0s - loss: 0.1399 - accuracy: 0.79 - 1s 2ms/sample - loss: 0.1391 - accuracy: 0.8000 - val_loss: 0.1864 - val_accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.87 - ETA: 0s - loss: 0.1305 - accuracy: 0.81 - ETA: 0s - loss: 0.1303 - accuracy: 0.79 - ETA: 0s - loss: 0.1333 - accuracy: 0.78 - ETA: 0s - loss: 0.1286 - accuracy: 0.79 - ETA: 0s - loss: 0.1132 - accuracy: 0.82 - ETA: 0s - loss: 0.1095 - accuracy: 0.84 - ETA: 0s - loss: 0.1073 - accuracy: 0.83 - ETA: 0s - loss: 0.1039 - accuracy: 0.85 - ETA: 0s - loss: 0.1038 - accuracy: 0.85 - ETA: 0s - loss: 0.1032 - accuracy: 0.85 - ETA: 0s - loss: 0.1059 - accuracy: 0.85 - 1s 2ms/sample - loss: 0.1113 - accuracy: 0.8484 - val_loss: 0.2190 - val_accuracy: 0.7688\n",
      "Epoch 7/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.75 - ETA: 0s - loss: 0.0841 - accuracy: 0.84 - ETA: 0s - loss: 0.0791 - accuracy: 0.87 - ETA: 0s - loss: 0.0917 - accuracy: 0.86 - ETA: 0s - loss: 0.0886 - accuracy: 0.86 - ETA: 0s - loss: 0.0911 - accuracy: 0.86 - ETA: 0s - loss: 0.0880 - accuracy: 0.86 - ETA: 0s - loss: 0.0884 - accuracy: 0.87 - ETA: 0s - loss: 0.0885 - accuracy: 0.87 - ETA: 0s - loss: 0.0868 - accuracy: 0.87 - ETA: 0s - loss: 0.0901 - accuracy: 0.87 - ETA: 0s - loss: 0.0912 - accuracy: 0.87 - 1s 2ms/sample - loss: 0.0901 - accuracy: 0.8750 - val_loss: 0.2201 - val_accuracy: 0.7688\n",
      "Epoch 8/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.96 - ETA: 0s - loss: 0.0740 - accuracy: 0.88 - ETA: 0s - loss: 0.0764 - accuracy: 0.86 - ETA: 0s - loss: 0.0780 - accuracy: 0.86 - ETA: 0s - loss: 0.0844 - accuracy: 0.86 - ETA: 0s - loss: 0.0830 - accuracy: 0.87 - ETA: 0s - loss: 0.0842 - accuracy: 0.87 - ETA: 0s - loss: 0.0825 - accuracy: 0.87 - ETA: 0s - loss: 0.0849 - accuracy: 0.87 - ETA: 0s - loss: 0.0822 - accuracy: 0.88 - ETA: 0s - loss: 0.0778 - accuracy: 0.88 - ETA: 0s - loss: 0.0765 - accuracy: 0.88 - ETA: 0s - loss: 0.0777 - accuracy: 0.88 - 1s 2ms/sample - loss: 0.0755 - accuracy: 0.8906 - val_loss: 0.2156 - val_accuracy: 0.7625\n",
      "Epoch 9/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.93 - ETA: 0s - loss: 0.0873 - accuracy: 0.89 - ETA: 0s - loss: 0.0888 - accuracy: 0.87 - ETA: 0s - loss: 0.0829 - accuracy: 0.89 - ETA: 0s - loss: 0.0775 - accuracy: 0.90 - ETA: 0s - loss: 0.0738 - accuracy: 0.90 - ETA: 0s - loss: 0.0677 - accuracy: 0.90 - ETA: 0s - loss: 0.0677 - accuracy: 0.91 - ETA: 0s - loss: 0.0676 - accuracy: 0.91 - ETA: 0s - loss: 0.0728 - accuracy: 0.90 - ETA: 0s - loss: 0.0688 - accuracy: 0.91 - ETA: 0s - loss: 0.0665 - accuracy: 0.91 - ETA: 0s - loss: 0.0665 - accuracy: 0.91 - ETA: 0s - loss: 0.0681 - accuracy: 0.91 - ETA: 0s - loss: 0.0683 - accuracy: 0.90 - ETA: 0s - loss: 0.0671 - accuracy: 0.90 - ETA: 0s - loss: 0.0678 - accuracy: 0.90 - 1s 2ms/sample - loss: 0.0658 - accuracy: 0.9094 - val_loss: 0.2107 - val_accuracy: 0.7750\n",
      "Epoch 10/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.90 - ETA: 0s - loss: 0.0482 - accuracy: 0.93 - ETA: 0s - loss: 0.0479 - accuracy: 0.94 - ETA: 0s - loss: 0.0607 - accuracy: 0.93 - ETA: 0s - loss: 0.0587 - accuracy: 0.94 - ETA: 0s - loss: 0.0581 - accuracy: 0.94 - ETA: 0s - loss: 0.0592 - accuracy: 0.93 - ETA: 0s - loss: 0.0633 - accuracy: 0.93 - ETA: 0s - loss: 0.0711 - accuracy: 0.91 - ETA: 0s - loss: 0.0704 - accuracy: 0.91 - ETA: 0s - loss: 0.0718 - accuracy: 0.91 - ETA: 0s - loss: 0.0693 - accuracy: 0.91 - ETA: 0s - loss: 0.0662 - accuracy: 0.92 - ETA: 0s - loss: 0.0657 - accuracy: 0.91 - 1s 2ms/sample - loss: 0.0671 - accuracy: 0.9172 - val_loss: 0.3707 - val_accuracy: 0.6250\n",
      "Epoch 11/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.84 - ETA: 0s - loss: 0.0820 - accuracy: 0.91 - ETA: 0s - loss: 0.0725 - accuracy: 0.91 - ETA: 0s - loss: 0.0940 - accuracy: 0.88 - ETA: 0s - loss: 0.0852 - accuracy: 0.89 - ETA: 0s - loss: 0.0775 - accuracy: 0.89 - ETA: 0s - loss: 0.0724 - accuracy: 0.90 - ETA: 0s - loss: 0.0698 - accuracy: 0.90 - ETA: 0s - loss: 0.0640 - accuracy: 0.91 - ETA: 0s - loss: 0.0620 - accuracy: 0.91 - ETA: 0s - loss: 0.0628 - accuracy: 0.91 - 1s 2ms/sample - loss: 0.0597 - accuracy: 0.9234 - val_loss: 0.2194 - val_accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "640/640 [==============================] - ETA: 1s - loss: 0.0562 - accuracy: 0.93 - ETA: 1s - loss: 0.0588 - accuracy: 0.90 - ETA: 1s - loss: 0.0474 - accuracy: 0.92 - ETA: 1s - loss: 0.0456 - accuracy: 0.93 - ETA: 0s - loss: 0.0466 - accuracy: 0.94 - ETA: 0s - loss: 0.0450 - accuracy: 0.94 - ETA: 0s - loss: 0.0416 - accuracy: 0.95 - ETA: 0s - loss: 0.0428 - accuracy: 0.95 - ETA: 0s - loss: 0.0391 - accuracy: 0.95 - ETA: 0s - loss: 0.0379 - accuracy: 0.95 - ETA: 0s - loss: 0.0395 - accuracy: 0.95 - ETA: 0s - loss: 0.0420 - accuracy: 0.95 - ETA: 0s - loss: 0.0413 - accuracy: 0.95 - ETA: 0s - loss: 0.0422 - accuracy: 0.95 - ETA: 0s - loss: 0.0413 - accuracy: 0.95 - ETA: 0s - loss: 0.0474 - accuracy: 0.94 - 1s 2ms/sample - loss: 0.0487 - accuracy: 0.9438 - val_loss: 0.2314 - val_accuracy: 0.7500\n",
      "Epoch 13/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.93 - ETA: 0s - loss: 0.0302 - accuracy: 0.96 - ETA: 0s - loss: 0.0316 - accuracy: 0.95 - ETA: 0s - loss: 0.0363 - accuracy: 0.95 - ETA: 0s - loss: 0.0394 - accuracy: 0.95 - ETA: 0s - loss: 0.0377 - accuracy: 0.95 - ETA: 0s - loss: 0.0401 - accuracy: 0.95 - ETA: 0s - loss: 0.0424 - accuracy: 0.94 - ETA: 0s - loss: 0.0445 - accuracy: 0.93 - ETA: 0s - loss: 0.0411 - accuracy: 0.94 - ETA: 0s - loss: 0.0416 - accuracy: 0.94 - ETA: 0s - loss: 0.0453 - accuracy: 0.93 - ETA: 0s - loss: 0.0448 - accuracy: 0.93 - ETA: 0s - loss: 0.0454 - accuracy: 0.93 - ETA: 0s - loss: 0.0459 - accuracy: 0.93 - 1s 2ms/sample - loss: 0.0468 - accuracy: 0.9359 - val_loss: 0.2233 - val_accuracy: 0.7750\n",
      "Epoch 14/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.96 - ETA: 0s - loss: 0.0466 - accuracy: 0.94 - ETA: 0s - loss: 0.0499 - accuracy: 0.94 - ETA: 0s - loss: 0.0563 - accuracy: 0.93 - ETA: 0s - loss: 0.0500 - accuracy: 0.93 - ETA: 0s - loss: 0.0502 - accuracy: 0.93 - ETA: 0s - loss: 0.0461 - accuracy: 0.94 - ETA: 0s - loss: 0.0416 - accuracy: 0.94 - ETA: 0s - loss: 0.0404 - accuracy: 0.94 - ETA: 0s - loss: 0.0457 - accuracy: 0.94 - ETA: 0s - loss: 0.0453 - accuracy: 0.93 - ETA: 0s - loss: 0.0440 - accuracy: 0.94 - ETA: 0s - loss: 0.0419 - accuracy: 0.94 - ETA: 0s - loss: 0.0414 - accuracy: 0.95 - ETA: 0s - loss: 0.0437 - accuracy: 0.94 - ETA: 0s - loss: 0.0430 - accuracy: 0.94 - 1s 2ms/sample - loss: 0.0455 - accuracy: 0.9453 - val_loss: 0.2152 - val_accuracy: 0.7750\n",
      "Epoch 15/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.96 - ETA: 0s - loss: 0.0351 - accuracy: 0.94 - ETA: 0s - loss: 0.0314 - accuracy: 0.95 - ETA: 0s - loss: 0.0309 - accuracy: 0.95 - ETA: 0s - loss: 0.0442 - accuracy: 0.94 - ETA: 0s - loss: 0.0443 - accuracy: 0.94 - ETA: 0s - loss: 0.0373 - accuracy: 0.95 - ETA: 0s - loss: 0.0356 - accuracy: 0.96 - ETA: 0s - loss: 0.0371 - accuracy: 0.95 - ETA: 0s - loss: 0.0431 - accuracy: 0.94 - ETA: 0s - loss: 0.0441 - accuracy: 0.94 - ETA: 0s - loss: 0.0434 - accuracy: 0.94 - ETA: 0s - loss: 0.0425 - accuracy: 0.94 - ETA: 0s - loss: 0.0420 - accuracy: 0.94 - 1s 2ms/sample - loss: 0.0415 - accuracy: 0.9516 - val_loss: 0.2618 - val_accuracy: 0.7188\n",
      "Epoch 16/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.93 - ETA: 0s - loss: 0.0261 - accuracy: 0.96 - ETA: 0s - loss: 0.0291 - accuracy: 0.96 - ETA: 0s - loss: 0.0406 - accuracy: 0.95 - ETA: 0s - loss: 0.0401 - accuracy: 0.94 - ETA: 0s - loss: 0.0352 - accuracy: 0.95 - ETA: 0s - loss: 0.0365 - accuracy: 0.95 - ETA: 0s - loss: 0.0352 - accuracy: 0.95 - ETA: 0s - loss: 0.0386 - accuracy: 0.95 - ETA: 0s - loss: 0.0371 - accuracy: 0.95 - ETA: 0s - loss: 0.0373 - accuracy: 0.95 - ETA: 0s - loss: 0.0356 - accuracy: 0.95 - 1s 2ms/sample - loss: 0.0359 - accuracy: 0.9531 - val_loss: 0.2278 - val_accuracy: 0.7625\n",
      "Epoch 17/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.96 - ETA: 0s - loss: 0.0457 - accuracy: 0.93 - ETA: 0s - loss: 0.0388 - accuracy: 0.95 - ETA: 0s - loss: 0.0366 - accuracy: 0.95 - ETA: 0s - loss: 0.0391 - accuracy: 0.94 - ETA: 0s - loss: 0.0423 - accuracy: 0.94 - ETA: 0s - loss: 0.0414 - accuracy: 0.94 - ETA: 0s - loss: 0.0386 - accuracy: 0.94 - ETA: 0s - loss: 0.0345 - accuracy: 0.95 - ETA: 0s - loss: 0.0323 - accuracy: 0.95 - ETA: 0s - loss: 0.0323 - accuracy: 0.95 - ETA: 0s - loss: 0.0365 - accuracy: 0.95 - 1s 2ms/sample - loss: 0.0361 - accuracy: 0.9563 - val_loss: 0.2082 - val_accuracy: 0.7750\n",
      "Epoch 18/20\n",
      "640/640 [==============================] - ETA: 1s - loss: 0.0256 - accuracy: 0.93 - ETA: 1s - loss: 0.0449 - accuracy: 0.93 - ETA: 1s - loss: 0.0348 - accuracy: 0.94 - ETA: 0s - loss: 0.0342 - accuracy: 0.96 - ETA: 0s - loss: 0.0377 - accuracy: 0.95 - ETA: 0s - loss: 0.0404 - accuracy: 0.95 - ETA: 0s - loss: 0.0422 - accuracy: 0.94 - ETA: 0s - loss: 0.0404 - accuracy: 0.94 - ETA: 0s - loss: 0.0397 - accuracy: 0.94 - ETA: 0s - loss: 0.0388 - accuracy: 0.94 - ETA: 0s - loss: 0.0370 - accuracy: 0.95 - ETA: 0s - loss: 0.0373 - accuracy: 0.94 - ETA: 0s - loss: 0.0361 - accuracy: 0.94 - ETA: 0s - loss: 0.0340 - accuracy: 0.95 - ETA: 0s - loss: 0.0330 - accuracy: 0.95 - ETA: 0s - loss: 0.0322 - accuracy: 0.95 - 1s 2ms/sample - loss: 0.0332 - accuracy: 0.9531 - val_loss: 0.2283 - val_accuracy: 0.7625\n",
      "Epoch 19/20\n",
      "640/640 [==============================] - ETA: 1s - loss: 0.0040 - accuracy: 1.00 - ETA: 1s - loss: 0.0265 - accuracy: 0.95 - ETA: 1s - loss: 0.0304 - accuracy: 0.95 - ETA: 0s - loss: 0.0285 - accuracy: 0.95 - ETA: 0s - loss: 0.0313 - accuracy: 0.95 - ETA: 0s - loss: 0.0288 - accuracy: 0.95 - ETA: 0s - loss: 0.0254 - accuracy: 0.96 - ETA: 0s - loss: 0.0238 - accuracy: 0.96 - ETA: 0s - loss: 0.0261 - accuracy: 0.96 - ETA: 0s - loss: 0.0287 - accuracy: 0.96 - ETA: 0s - loss: 0.0294 - accuracy: 0.96 - ETA: 0s - loss: 0.0284 - accuracy: 0.96 - ETA: 0s - loss: 0.0276 - accuracy: 0.96 - 1s 2ms/sample - loss: 0.0263 - accuracy: 0.9656 - val_loss: 0.2269 - val_accuracy: 0.7625\n",
      "Epoch 20/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 1.00 - ETA: 0s - loss: 0.0180 - accuracy: 1.00 - ETA: 0s - loss: 0.0275 - accuracy: 0.98 - ETA: 0s - loss: 0.0232 - accuracy: 0.98 - ETA: 0s - loss: 0.0235 - accuracy: 0.97 - ETA: 0s - loss: 0.0307 - accuracy: 0.97 - ETA: 0s - loss: 0.0283 - accuracy: 0.97 - ETA: 0s - loss: 0.0311 - accuracy: 0.96 - ETA: 0s - loss: 0.0298 - accuracy: 0.96 - ETA: 0s - loss: 0.0328 - accuracy: 0.96 - ETA: 0s - loss: 0.0339 - accuracy: 0.96 - ETA: 0s - loss: 0.0351 - accuracy: 0.96 - 1s 2ms/sample - loss: 0.0357 - accuracy: 0.9625 - val_loss: 0.2475 - val_accuracy: 0.7375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2aa9d622198>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=32,epochs=20,validation_data=[X_val,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       106\n",
      "           1       0.76      0.73      0.75        94\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       200\n",
      "   macro avg       0.76      0.76      0.76       200\n",
      "weighted avg       0.76      0.77      0.76       200\n",
      "\n",
      "0.765\n"
     ]
    }
   ],
   "source": [
    "preds=np.round(model.predict(X_test))\n",
    "print(classification_report(y_test,preds))\n",
    "print(accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData('LearntEmb.pkl',emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Jointly Learnt Word Embeddings to Form Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6646 is out of bounds for axis 0 with size 6646",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-cdd893434252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msum_words\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_words\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6646 is out of bounds for axis 0 with size 6646"
     ]
    }
   ],
   "source": [
    "emb=loadData('LearntEmb.pkl')\n",
    "X=[]\n",
    "for r in revs:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        sum_words+=emb[w]\n",
    "    X.append(sum_words/num_words)\n",
    "X=np.array(X)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.57      0.62       100\n",
      "           1       0.44      0.57      0.50        60\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       160\n",
      "   macro avg       0.56      0.57      0.56       160\n",
      "weighted avg       0.59      0.57      0.58       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.65      0.63       100\n",
      "           1       0.34      0.30      0.32        60\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       160\n",
      "   macro avg       0.47      0.47      0.47       160\n",
      "weighted avg       0.51      0.52      0.51       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.63      0.64       100\n",
      "           1       0.40      0.42      0.41        60\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       160\n",
      "   macro avg       0.52      0.52      0.52       160\n",
      "weighted avg       0.55      0.55      0.55       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Word Vectors with TF-IDF to form Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_learnt = [] # the tfidf-sk for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(20) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = emb[word2id[word]]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_learnt.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91        92\n",
      "           1       0.89      0.85      0.87        68\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       160\n",
      "   macro avg       0.89      0.89      0.89       160\n",
      "weighted avg       0.89      0.89      0.89       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87        92\n",
      "           1       0.88      0.74      0.80        68\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       160\n",
      "   macro avg       0.85      0.83      0.84       160\n",
      "weighted avg       0.85      0.84      0.84       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87        92\n",
      "           1       0.85      0.76      0.81        68\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       160\n",
      "   macro avg       0.85      0.83      0.84       160\n",
      "weighted avg       0.84      0.84      0.84       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_skip=np.array(tfidf_sent_vectors_learnt)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_skip,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just TF-IDF Features for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf=final_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_tfidf,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83        90\n",
      "           1       0.81      0.69      0.74        70\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       160\n",
      "   macro avg       0.80      0.78      0.79       160\n",
      "weighted avg       0.80      0.79      0.79       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77        90\n",
      "           1       0.73      0.57      0.64        70\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       160\n",
      "   macro avg       0.72      0.70      0.70       160\n",
      "weighted avg       0.72      0.72      0.71       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        90\n",
      "           1       0.81      0.77      0.79        70\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       160\n",
      "   macro avg       0.82      0.81      0.81       160\n",
      "weighted avg       0.82      0.82      0.82       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 15:24:56.224439 12412 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model_d2v = Doc2Vec(documents,dm=1,vector_size=20, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_docs=model_d2v.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        86\n",
      "           1       0.00      0.00      0.00        74\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       160\n",
      "   macro avg       0.27      0.50      0.35       160\n",
      "weighted avg       0.29      0.54      0.38       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.78      0.64        86\n",
      "           1       0.49      0.24      0.32        74\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       160\n",
      "   macro avg       0.52      0.51      0.48       160\n",
      "weighted avg       0.52      0.53      0.49       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.72      0.63        86\n",
      "           1       0.52      0.35      0.42        74\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       160\n",
      "   macro avg       0.54      0.54      0.53       160\n",
      "weighted avg       0.54      0.55      0.53       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_docs,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 15:25:02.013295 12412 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model_d2v = Doc2Vec(documents,dm=0,vector_size=20, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_docs=model_d2v.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.71        89\n",
      "           1       0.00      0.00      0.00        71\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       160\n",
      "   macro avg       0.28      0.50      0.36       160\n",
      "weighted avg       0.31      0.56      0.40       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63        89\n",
      "           1       0.42      0.24      0.31        71\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       160\n",
      "   macro avg       0.49      0.49      0.47       160\n",
      "weighted avg       0.49      0.52      0.49       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60        89\n",
      "           1       0.45      0.37      0.40        71\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       160\n",
      "   macro avg       0.50      0.50      0.50       160\n",
      "weighted avg       0.51      0.52      0.51       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_docs,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elmo Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(tokenizer.index_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveData('Words.pkl',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=loadData('Words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_word_embs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "b_s=32\n",
    "for i in range(177,208):\n",
    "    elmo_word_embs.append(elmo_vectors(words[i*b_s:i*b_s+b_s]))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_word_embs=np.concatenate(elmo_word_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveData('ElmoWordEmb.pkl',word_embs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_full=loadData('ElmoWordEmb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\" \".join(i) for i in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6646, 1024)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.47999403, -0.00448739, -0.13489588, ..., -0.2204245 ,\n",
       "       -0.19793135,  0.01283459], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs_full[words.index('hi')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging ELMo word representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x6646 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30365 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_g=[]\n",
    "for r in reviews:\n",
    "    num_words=len(r)\n",
    "    sum_words=0\n",
    "    for w in r:\n",
    "        if w != '-PRON-':\n",
    "            sum_words+=word_embs_full[words.index(w)]\n",
    "    X_g.append(sum_words/num_words) \n",
    "X_g=np.array(X_g)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_g,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75        81\n",
      "           1       0.75      0.72      0.74        79\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       160\n",
      "   macro avg       0.74      0.74      0.74       160\n",
      "weighted avg       0.74      0.74      0.74       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.84      0.71        81\n",
      "           1       0.74      0.47      0.57        79\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       160\n",
      "   macro avg       0.68      0.65      0.64       160\n",
      "weighted avg       0.68      0.66      0.64       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78        81\n",
      "           1       0.79      0.72      0.75        79\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       160\n",
      "   macro avg       0.77      0.77      0.77       160\n",
      "weighted avg       0.77      0.77      0.77       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining ELMo word representations with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors noted: 1\n"
     ]
    }
   ],
   "source": [
    "tfidf_sent_vectors_elmo= [] # the tfidf-sk for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "for sent in reviews: # for each review/sentence\n",
    "    sent_vec = np.zeros(1024) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = word_embs_full[words.index(word)]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "\n",
    "    tfidf_sent_vectors_elmo.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.78        77\n",
      "           1       0.83      0.70      0.76        83\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       160\n",
      "   macro avg       0.78      0.77      0.77       160\n",
      "weighted avg       0.78      0.77      0.77       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64        77\n",
      "           1       0.67      0.46      0.54        83\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       160\n",
      "   macro avg       0.61      0.61      0.59       160\n",
      "weighted avg       0.62      0.60      0.59       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74        77\n",
      "           1       0.79      0.63      0.70        83\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       160\n",
      "   macro avg       0.73      0.72      0.72       160\n",
      "weighted avg       0.73      0.72      0.72       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_skip=np.array(tfidf_sent_vectors_elmo)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_skip,y,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(X_train,y_train)\n",
    "preds_valid = lreg.predict(X_val)\n",
    "print(classification_report(y_val,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "preds=rf.predict(X_val)\n",
    "print(classification_report(y_val,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "preds=xgb.predict(X_val)\n",
    "print(classification_report(y_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo Doc Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "elm_emb=loadData('ElmoEmbeddings.pkl')\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(elm_emb, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       115\n",
      "           1       0.86      0.84      0.85        85\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       200\n",
      "   macro avg       0.87      0.87      0.87       200\n",
      "weighted avg       0.87      0.87      0.87       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       115\n",
      "           1       0.77      0.84      0.80        85\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       200\n",
      "   macro avg       0.82      0.83      0.82       200\n",
      "weighted avg       0.83      0.82      0.83       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.76       115\n",
      "           1       0.68      0.59      0.63        85\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       200\n",
      "   macro avg       0.70      0.69      0.69       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain, ytrain)\n",
    "preds_valid = lreg.predict(xvalid)\n",
    "print(classification_report(yvalid,preds_valid))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(xtrain,ytrain)\n",
    "preds=xgb.predict(xvalid)\n",
    "print(classification_report(yvalid,preds))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(xtrain,ytrain)\n",
    "preds=rf.predict(xvalid)\n",
    "print(classification_report(yvalid,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Doc Representations\n",
    "Requires Pytorch and pytorch BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "X=df.review[:1000].apply(str.replace,args=('.','. [SEP] ')).values\n",
    "marked_X=[]\n",
    "for x in X:\n",
    "    marked_X.append('[CLS] '+x+ (' [SEP]' if x[-6:]!='[SEP] ' else \" \"))\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized_texts=[]\n",
    "for x in marked_X:\n",
    "    tokenized_text = tokenizer.tokenize(x)\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "\n",
    "tokenized_indexed_texts=[]\n",
    "for x in tokenized_texts:\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(x)\n",
    "    tokenized_indexed_texts.append(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Embedding using average of the second last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings=[]\n",
    "ctr=0\n",
    "for t in np.array(tokenized_indexed_texts):\n",
    "    ctr+=1\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([t])\n",
    "    segments_tensors = torch.tensor([[1]*len(t)])\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    sentence_embedding = torch.mean(encoded_layers[11], 1)\n",
    "    doc_embeddings.append(np.array(sentence_embedding.tolist()))\n",
    "    if ctr%30==0:\n",
    "        print(ctr)\n",
    "#saveData('BertEmbWOPad.pkl',doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       115\n",
      "           1       0.90      0.85      0.87        85\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       200\n",
      "   macro avg       0.90      0.89      0.89       200\n",
      "weighted avg       0.90      0.90      0.89       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       115\n",
      "           1       0.82      0.74      0.78        85\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       200\n",
      "   macro avg       0.82      0.81      0.81       200\n",
      "weighted avg       0.82      0.82      0.82       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88       115\n",
      "           1       0.85      0.84      0.84        85\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       200\n",
      "   macro avg       0.86      0.86      0.86       200\n",
      "weighted avg       0.86      0.86      0.86       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_embeddings=loadData('BertEmbWOPad.pkl')\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(doc_embeddings, y, random_state=42, test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain, ytrain)\n",
    "preds_valid = lreg.predict(xvalid)\n",
    "print(classification_report(yvalid,preds_valid))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(xtrain, ytrain)\n",
    "preds=rf.predict(xvalid)\n",
    "print(classification_report(yvalid,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(xtrain, ytrain)\n",
    "preds=xgb.predict(xvalid)\n",
    "print(classification_report(yvalid,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Embedding using average of Token Embeddings (formed by concatenating last 4 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_token_embeddings=[]\n",
    "ctr=0\n",
    "for t in np.array(tokenized_indexed_texts):\n",
    "    ctr+=1\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([t])\n",
    "    segments_tensors = torch.tensor([[1]*len(t)])\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    token_embeddings = [] \n",
    "    # For each token in the sentence...\n",
    "    for token_i in range(len(t)):\n",
    "        # Holds 12 layers of hidden states for each token \n",
    "        hidden_layers = [] \n",
    "        # For each of the 12 layers...\n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "            # Lookup the vector for `token_i` in `layer_i`\n",
    "            vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "            hidden_layers.append(vec)\n",
    "        token_embeddings.append(hidden_layers)\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat = []\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat.append(np.array([cat_vec.tolist()]))\n",
    "    doc_token_embeddings.append(np.concatenate(token_vecs_cat).mean(axis=0))\n",
    "    if ctr%30==0:\n",
    "        print(ctr)\n",
    "saveData('DocTokenWOPadBert.pkl',np.array(doc_token_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       115\n",
      "           1       0.90      0.85      0.87        85\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       200\n",
      "   macro avg       0.90      0.89      0.89       200\n",
      "weighted avg       0.90      0.90      0.89       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84       115\n",
      "           1       0.79      0.73      0.76        85\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       200\n",
      "   macro avg       0.80      0.80      0.80       200\n",
      "weighted avg       0.80      0.81      0.80       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       115\n",
      "           1       0.86      0.89      0.88        85\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       200\n",
      "   macro avg       0.89      0.89      0.89       200\n",
      "weighted avg       0.90      0.90      0.90       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_token_embeddings=loadData('DocTokenWOPadBert.pkl')\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(doc_token_embeddings, y, random_state=42, test_size=0.2)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain, ytrain)\n",
    "preds_valid = lreg.predict(xvalid)\n",
    "print(classification_report(yvalid,preds_valid))\n",
    "\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(xtrain, ytrain)\n",
    "preds=rf.predict(xvalid)\n",
    "print(classification_report(yvalid,preds))\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(xtrain, ytrain)\n",
    "preds=xgb.predict(xvalid)\n",
    "print(classification_report(yvalid,preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
